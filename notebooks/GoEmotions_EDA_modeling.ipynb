{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoEmotions: A Fine-Grained Multi-Label Emotion Classifier\n",
    "# Project Overview\n",
    "\n",
    "This notebook details a capstone project focused on building a multi-label emotion classification system. The primary goal is to predict one or more of 28 distinct emotion labels from a given Reddit comment. This project navigates key challenges of the task, including class imbalance and the multi-label nature of the data, to build a high-performing deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Data Loading\n",
    "We begin by importing the necessary Python libraries. These libraries are used for data manipulation, visualization, text processing, and deep learning model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas pyarrow nltk emoji numpy matplotlib seaborn wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the pre-split GoEmotions dataset. This dataset is sourced from Reddit and has been carefully annotated by human raters with fine-grained emotion labels. We perform an initial check on the dataset splits to verify their size and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the Simplified Splits\n",
    "splits = {\n",
    "    'train':      'simplified/train-00000-of-00001.parquet',\n",
    "    'validation': 'simplified/validation-00000-of-00001.parquet',\n",
    "    'test':       'simplified/test-00000-of-00001.parquet'\n",
    "}\n",
    "base = \"hf://datasets/google-research-datasets/go_emotions/\"\n",
    "\n",
    "df_train = pd.read_parquet(base + splits['train'])\n",
    "df_val   = pd.read_parquet(base + splits['validation'])\n",
    "df_test  = pd.read_parquet(base + splits['test'])\n",
    "\n",
    "print(\"Train examples:\", len(df_train))\n",
    "print(\"Validation examples:\", len(df_val))\n",
    "print(\"Test examples:\", len(df_test))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis (EDA)\n",
    "Data exploration is a critical step in understanding the unique challenges of the GoEmotions dataset. The findings here will inform our model's design, particularly the choice of evaluation metrics and loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Label Cardinality & Distribution\n",
    "First, we examine label cardinality, which is the number of emotions assigned to a single comment. The dataset is multi-label, and this analysis reveals that while most comments have a single label, a notable portion has two or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = df_train['labels'].map(len)\n",
    "print(\"Label Cardinality:\")\n",
    "print(card.value_counts().sort_index())\n",
    "\n",
    "val_card = df_val['labels'].map(len)\n",
    "print(\"\\nLabel Cardinality in Validation Set:\")\n",
    "print(val_card.value_counts().sort_index())\n",
    "\n",
    "test_card = df_test['labels'].map(len)\n",
    "print(\"\\nLabel Cardinality in Test Set:\")\n",
    "print(test_card.value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the frequency of each emotion. The bar chart clearly shows a severe class imbalance. Emotions like admiration and gratitude are very common, while others like grief and pride are rare. This finding is critical for our modeling approach, as it necessitates the use of a specialized loss function like Focal Loss to prevent the model from ignoring the less frequent emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "all_labels = [lbl for labels in df_train['labels'] for lbl in labels]\n",
    "freq = Counter(all_labels)\n",
    "counts = [freq[i] for i in range(len(emotion_labels))]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(emotion_labels, counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('GoEmotions Label Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a higher-level view, we can group the 28 fine-grained emotions into broader sentiment categories: positive, negative, and ambiguous. This provides a simplified perspective on the overall emotional tone of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize emotions\n",
    "positive = {\n",
    "    'admiration', 'amusement', 'approval', 'caring', 'excitement', 'gratitude',\n",
    "    'joy', 'love', 'optimism', 'pride', 'relief', 'desire'\n",
    "}\n",
    "negative = {\n",
    "    'anger', 'annoyance', 'disappointment', 'disapproval', 'disgust',\n",
    "    'embarrassment', 'fear', 'grief', 'nervousness', 'remorse', 'sadness'\n",
    "}\n",
    "ambiguous = {\n",
    "    'confusion', 'curiosity', 'realization', 'surprise', 'neutral'\n",
    "}\n",
    "\n",
    "# Build a map from label index to category\n",
    "label_to_category = {}\n",
    "for idx, label in enumerate(emotion_labels):\n",
    "    if label in positive:\n",
    "        label_to_category[idx] = 'positive'\n",
    "    elif label in negative:\n",
    "        label_to_category[idx] = 'negative'\n",
    "    else:\n",
    "        label_to_category[idx] = 'ambiguous'\n",
    "\n",
    "# Count category frequencies\n",
    "category_counts = Counter()\n",
    "for labels in df_train['labels']:\n",
    "    for lbl in labels:\n",
    "        category = label_to_category[lbl]\n",
    "        category_counts[category] += 1\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(category_counts.keys(), category_counts.values(), color=['green', 'blue', 'red'])\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('GoEmotions Sentiment Category Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we analyze co-occurring emotion combinations to understand the relationships between the labels. This is a key feature of a multi-label dataset and can provide insights for more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comments with multiple labels\n",
    "multi_label_comments = df_train[df_train['labels'].apply(len) > 1]\n",
    "\n",
    "# Analyze co-occurring emotions (for comments with more than one label)\n",
    "co_occurrence = Counter()\n",
    "for labels in multi_label_comments['labels']:\n",
    "    # Sort labels to count combinations uniquely\n",
    "    sorted_labels = tuple(sorted(labels))\n",
    "    co_occurrence[sorted_labels] += 1\n",
    "\n",
    "print(\"Most common co-occurring emotion combinations:\")\n",
    "# Map label indices back to emotion names for readability\n",
    "most_common_co_occurrences = co_occurrence.most_common(20)\n",
    "for combo, count in most_common_co_occurrences:\n",
    "    emotion_combo = [emotion_labels[i] for i in combo]\n",
    "    print(f\"{emotion_combo}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text Characteristics & Cleaning Needs\n",
    "Examining the text's properties helps us design a robust preprocessing pipeline. We analyze the distribution of text lengths and identify the presence of placeholders and non-ASCII characters, which are forms of noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"String length (chars) stats:\")\n",
    "df_train['stringlengths'] = df_train['text'].str.len()\n",
    "print(df_train['stringlengths'].describe(percentiles=[0.25, 0.5, 0.75, 0.95]))\n",
    "\n",
    "name_placeholder_count = df_train['text'].str.contains(r'\\[NAME\\]').sum()\n",
    "non_ascii_count = df_train['text'].str.contains(r'[^\\x00-\\x7F]').sum()\n",
    "\n",
    "print(f\"\\n[NAME] placeholders: {name_placeholder_count}\")\n",
    "print(f\"Comments with nonâ€‘ASCII chars: {non_ascii_count}\")\n",
    "\n",
    "labels = ['[NAME] Placeholders', 'Comments with Non-ASCII']\n",
    "counts = [name_placeholder_count, non_ascii_count]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, counts)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Placeholders and Non-ASCII Characters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing and Tokenization\n",
    "Based on our EDA findings, we create a cleaning and tokenization pipeline. This prepares the raw text for a GloVe-based model by standardizing contractions, handling emojis, removing noise, and lemmatizing words to their base form. Below is a summary of the functions we'll use for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_emoji_map(df):\n",
    "    all_nonascii = df['text'].str.findall(r'[^\\\\x00-\\\\x7F]').explode().dropna()\n",
    "    all_emojis   = [ch for ch in all_nonascii if ch in emoji.EMOJI_DATA]\n",
    "    top_50       = [emo for emo, _ in Counter(all_emojis).most_common(50)]\n",
    "\n",
    "    emoji_map = {}\n",
    "    for emo in top_50:\n",
    "        desc = emoji.demojize(emo).strip(':').replace('/', '_')\n",
    "        emoji_map[emo] = f\" {desc}_emoji \"\n",
    "    return emoji_map\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # 1) Placeholder & HTML\n",
    "    text = text.replace('[NAME]', ' name_token ')\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "    # 2) Standardize contractions\n",
    "    for patt, repl in {\n",
    "        r\"\\bI'm\\b\": \"i 'm\", r\"\\bit's\\b\": \"it 's\", r\"\\bcan't\\b\": \"ca n't\",\n",
    "        r\"\\bdon't\\b\": \"do n't\", r\"\\bthey're\\b\": \"they 're\"\n",
    "    }.items():\n",
    "        text = re.sub(patt, repl, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 3) Handle hashtags & handles\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'@\\w+', ' user_token ', text)\n",
    "\n",
    "    # 4) Map or preserve emoticons/emojis\n",
    "    for emo, tok in emoji_map.items():\n",
    "        text = text.replace(emo, f' {tok} ')\n",
    "\n",
    "    # 5) Remove non-ASCII, collapse repeats\n",
    "    text = re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "    # 6) Lowercase & strip unwanted\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z'\\s_#@]\", ' ', text)\n",
    "\n",
    "    # 7) Tokenize & lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t, pos='v') if t.isalpha() else t for t in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "emoji_map = get_emoji_map(df_train)\n",
    "\n",
    "df_train['tokens'] = df_train['text'].map(lambda x: clean_and_tokenize(x))\n",
    "df_val  ['tokens'] = df_val  ['text'].map(lambda x: clean_and_tokenize(x))\n",
    "df_test ['tokens'] = df_test ['text'].map(lambda x: clean_and_tokenize(x))\n",
    "\n",
    "print(\"Example tokens:\", df_train.loc[0, 'tokens'])\n",
    "print(\"Average token count (train):\", df_train['tokens'].map(len).mean())\n",
    "print(\"75th-percentile token count:\", df_train['tokens'].map(len).quantile(0.75))\n",
    "\n",
    "# Visual Check â€“ Before vs After (10 samples)\n",
    "comparison_df = df_train[['text', 'tokens']].sample(10, random_state=42)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    print(f\"\\nOriginal ({i}): {row['text']}\")\n",
    "    print(f\"Cleaned+Lemmatized ({i}): {row['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use word clouds to visually inspect the most common words associated with each sentiment category. This provides qualitative insights into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine tokens for each sentiment category\n",
    "positive_tokens = [token for tokens in df_train[df_train['labels'].apply(lambda x: any(label_to_category[lbl] == 'positive' for lbl in x))]['tokens'] for token in tokens]\n",
    "negative_tokens = [token for tokens in df_train[df_train['labels'].apply(lambda x: any(label_to_category[lbl] == 'negative' for lbl in x))]['tokens'] for token in tokens]\n",
    "ambiguous_tokens = [token for tokens in df_train[df_train['labels'].apply(lambda x: any(label_to_category[lbl] == 'ambiguous' for lbl in x))]['tokens'] for token in tokens]\n",
    "\n",
    "# Generate word clouds\n",
    "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(positive_tokens))\n",
    "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(negative_tokens))\n",
    "wordcloud_ambiguous = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(ambiguous_tokens))\n",
    "\n",
    "# Display the word clouds\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
    "plt.title('Positive Emotions Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
    "plt.title('Negative Emotions Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(wordcloud_ambiguous, interpolation='bilinear')\n",
    "plt.title('Ambiguous Emotions Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
